# ç¬¬äº”ç« å‰ç½®çŸ¥è¯†è¡¥å……ï¼šä» Transformer åˆ° GPT

> **ç›®æ ‡è¯»è€…**ï¼šå®Œæˆç¬¬äºŒç«  Transformer å­¦ä¹ ï¼Œå‡†å¤‡è¿›å…¥ç¬¬äº”ç« "åŠ¨æ‰‹æ­å»ºå¤§æ¨¡å‹"çš„ Beginner  
> **å‚è€ƒèµ„æ–™**ï¼š[Datawhale Happy-LLM](https://datawhalechina.github.io/happy-llm/)  
> **ä½œè€…**ï¼šAI Assistant  
> **æ›´æ–°æ—¥æœŸ**ï¼š2025-11-05

---

## ğŸ“‹ ç›®å½•

1. [å­¦ä¹ è·¯çº¿å›¾](#å­¦ä¹ è·¯çº¿å›¾)
2. [æ ¸å¿ƒæ¦‚å¿µ 1ï¼šDecoder-only æ¶æ„](#æ ¸å¿ƒæ¦‚å¿µ-1decoder-only-æ¶æ„)
3. [æ ¸å¿ƒæ¦‚å¿µ 2ï¼šKV Cacheï¼ˆé‡ç‚¹ï¼‰](#æ ¸å¿ƒæ¦‚å¿µ-2kv-cacheé‡ç‚¹)
4. [æ ¸å¿ƒæ¦‚å¿µ 3ï¼šRoPE ä½ç½®ç¼–ç ](#æ ¸å¿ƒæ¦‚å¿µ-3rope-ä½ç½®ç¼–ç )
5. [æ ¸å¿ƒæ¦‚å¿µ 4ï¼šSwiGLU æ¿€æ´»å‡½æ•°](#æ ¸å¿ƒæ¦‚å¿µ-4swiglu-æ¿€æ´»å‡½æ•°)
6. [æ ¸å¿ƒæ¦‚å¿µ 5ï¼šRMS Norm](#æ ¸å¿ƒæ¦‚å¿µ-5rms-norm)
7. [å®Œæ•´ä»£ç ç¤ºä¾‹](#å®Œæ•´ä»£ç ç¤ºä¾‹)
8. [å¸¸è§é—®é¢˜ FAQ](#å¸¸è§é—®é¢˜-faq)

---

## å­¦ä¹ è·¯çº¿å›¾

```
ä½ å·²ç»å­¦è¿‡çš„ï¼ˆç¬¬äºŒç« ï¼‰          ç¬¬äº”ç« ä¼šé‡åˆ°çš„              æœ¬æ–‡æ¡£å¸®ä½ è¡¥å……çš„
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Transformer    â”‚         â”‚      GPT        â”‚         â”‚  Decoder-only   â”‚
â”‚  (å®Œæ•´æ¶æ„)      â”‚   â†’     â”‚  (Decoder-only) â”‚   â†â”€    â”‚  æ¶æ„è¯¦è§£       â”‚
â”‚                 â”‚         â”‚                 â”‚         â”‚                 â”‚
â”‚  Multi-Head     â”‚         â”‚  Grouped Query  â”‚         â”‚  KV Cache       â”‚
â”‚  Attention      â”‚   â†’     â”‚  Attention      â”‚   â†â”€    â”‚  æœºåˆ¶è¯¦è§£       â”‚
â”‚                 â”‚         â”‚                 â”‚         â”‚                 â”‚
â”‚  Position       â”‚         â”‚      RoPE       â”‚         â”‚  RoPE ç®€åŒ–      â”‚
â”‚  Encoding       â”‚   â†’     â”‚  (æ—‹è½¬ç¼–ç )      â”‚   â†â”€    â”‚  è®²è§£           â”‚
â”‚                 â”‚         â”‚                 â”‚         â”‚                 â”‚
â”‚  ReLU + FFN     â”‚         â”‚     SwiGLU      â”‚         â”‚  æ¿€æ´»å‡½æ•°       â”‚
â”‚                 â”‚   â†’     â”‚                 â”‚   â†â”€    â”‚  æ¼”åŒ–å²         â”‚
â”‚                 â”‚         â”‚                 â”‚         â”‚                 â”‚
â”‚  Layer Norm     â”‚         â”‚    RMS Norm     â”‚         â”‚  å½’ä¸€åŒ–æ–¹æ³•     â”‚
â”‚                 â”‚   â†’     â”‚                 â”‚   â†â”€    â”‚  å¯¹æ¯”           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## æ ¸å¿ƒæ¦‚å¿µ 1ï¼šDecoder-only æ¶æ„

### ğŸ¯ ä¸ºä»€ä¹ˆè¦ Decoder-onlyï¼Ÿ

å›å¿†ä¸€ä¸‹ä½ åœ¨ç¬¬äºŒç« å­¦è¿‡çš„å®Œæ•´ Transformerï¼š

```
å®Œæ•´ Transformer = Encoder + Decoder

Encoderï¼ˆç¼–ç å™¨ï¼‰:
- è¾“å…¥ï¼šæºè¯­è¨€å¥å­ "I love you"
- è¾“å‡ºï¼šç†è§£åçš„è¡¨ç¤ºå‘é‡
- ç‰¹ç‚¹ï¼šåŒå‘æ³¨æ„åŠ›ï¼ˆå¯ä»¥çœ‹å‰åæ‰€æœ‰è¯ï¼‰

Decoderï¼ˆè§£ç å™¨ï¼‰:
- è¾“å…¥ï¼šç›®æ ‡è¯­è¨€å¼€å¤´ "æˆ‘"
- è¾“å‡ºï¼šé¢„æµ‹ä¸‹ä¸€ä¸ªè¯ "çˆ±"
- ç‰¹ç‚¹ï¼šå•å‘æ³¨æ„åŠ›ï¼ˆåªèƒ½çœ‹å‰é¢çš„è¯ï¼Œä¸èƒ½å·çœ‹åé¢ï¼‰
```

**GPT çš„é€‰æ‹©**ï¼šåªç”¨ Decoderï¼

```
GPT = Decoder + Decoder + Decoder + ... (åªå †å  Decoder å±‚)

ä¸ºä»€ä¹ˆï¼Ÿ
âœ… ä»»åŠ¡æ˜¯"æ–‡æœ¬ç”Ÿæˆ"ï¼ˆé¢„æµ‹ä¸‹ä¸€ä¸ªè¯ï¼‰
âœ… ç”Ÿæˆæ—¶ä¸éœ€è¦"ç†è§£"å¦ä¸€ç§è¯­è¨€ï¼ˆä¸éœ€è¦ Encoderï¼‰
âœ… åªéœ€è¦æ ¹æ®å‰æ–‡é¢„æµ‹åæ–‡ï¼ˆDecoder å°±å¤Ÿäº†ï¼‰
```

### ğŸ“Š ä¸‰ç§æ¶æ„å¯¹æ¯”

| ç‰¹æ€§           | Encoder-only<br>(BERT) | Decoder-only<br>(GPT) | Encoder-Decoder<br>(T5)      |
| -------------- | ---------------------- | --------------------- | ---------------------------- |
| **æ³¨æ„åŠ›æ–¹å¼** | åŒå‘ï¼ˆçœ‹æ‰€æœ‰è¯ï¼‰       | å•å‘ï¼ˆåªçœ‹å‰é¢ï¼‰      | Encoder åŒå‘ + Decoder å•å‘  |
| **Mask çŸ©é˜µ**  | âŒ ä¸éœ€è¦              | âœ… éœ€è¦ï¼ˆä¸Šä¸‰è§’ï¼‰     | Encoder ä¸éœ€è¦ï¼ŒDecoder éœ€è¦ |
| **æ“…é•¿ä»»åŠ¡**   | æ–‡æœ¬åˆ†ç±»ã€NER          | æ–‡æœ¬ç”Ÿæˆã€å¯¹è¯        | ç¿»è¯‘ã€æ‘˜è¦                   |
| **ä»£è¡¨æ¨¡å‹**   | BERT, RoBERTa          | GPT-2/3/4, LLaMA      | T5, BART                     |
| **è®­ç»ƒç›®æ ‡**   | MLMï¼ˆå¡«ç©ºï¼‰            | CLMï¼ˆé¢„æµ‹ä¸‹ä¸€è¯ï¼‰     | Seq2Seq                      |

### ğŸ” å…³é”®ä»£ç å¯¹æ¯”

**ä½ ç¬¬äºŒç« å­¦è¿‡çš„ `is_causal` å‚æ•°**ï¼š

```python
class MultiHeadAttention(nn.Module):
    def __init__(self, args: ModelArgs, is_causal=False):
        #            â†‘ è¿™ä¸ªå‚æ•°å†³å®šæ˜¯ Encoder è¿˜æ˜¯ Decoderï¼
        super().__init__()
        self.is_causal = is_causal

        if is_causal:
            # Decoder éœ€è¦ maskï¼ˆåªèƒ½çœ‹å‰é¢çš„è¯ï¼‰
            mask = torch.full((1, 1, max_len, max_len), float("-inf"))
            mask = torch.triu(mask, diagonal=1)  # ä¸Šä¸‰è§’
            self.register_buffer("mask", mask)
```

**BERT (Encoder-only)**ï¼š`is_causal=False`  
**GPT (Decoder-only)**ï¼š`is_causal=True`

**Mask çš„ä½œç”¨**ï¼ˆä½ ç¬¬äºŒç« å­¦è¿‡çš„ï¼‰ï¼š

```
å‡è®¾è¾“å…¥ï¼š"ä»Šå¤© å¤©æ°” å¾ˆ å¥½"

æ²¡æœ‰ Maskï¼ˆBERTï¼‰:
ä»Šå¤© â†’ å¯ä»¥çœ‹åˆ°ï¼šä»Šå¤©, å¤©æ°”, å¾ˆ, å¥½  âœ… åŒå‘
å¤©æ°” â†’ å¯ä»¥çœ‹åˆ°ï¼šä»Šå¤©, å¤©æ°”, å¾ˆ, å¥½  âœ… åŒå‘
å¾ˆ   â†’ å¯ä»¥çœ‹åˆ°ï¼šä»Šå¤©, å¤©æ°”, å¾ˆ, å¥½  âœ… åŒå‘
å¥½   â†’ å¯ä»¥çœ‹åˆ°ï¼šä»Šå¤©, å¤©æ°”, å¾ˆ, å¥½  âœ… åŒå‘

æœ‰ Maskï¼ˆGPTï¼‰:
ä»Šå¤© â†’ åªèƒ½çœ‹åˆ°ï¼šä»Šå¤©               âœ… å•å‘
å¤©æ°” â†’ åªèƒ½çœ‹åˆ°ï¼šä»Šå¤©, å¤©æ°”         âœ… å•å‘
å¾ˆ   â†’ åªèƒ½çœ‹åˆ°ï¼šä»Šå¤©, å¤©æ°”, å¾ˆ     âœ… å•å‘
å¥½   â†’ åªèƒ½çœ‹åˆ°ï¼šä»Šå¤©, å¤©æ°”, å¾ˆ, å¥½ âœ… å•å‘
```

---

## æ ¸å¿ƒæ¦‚å¿µ 2ï¼šKV Cacheï¼ˆé‡ç‚¹ï¼‰

> âš ï¸ **è¿™æ˜¯ç¬¬äº”ç« æœ€éš¾ç†è§£çš„éƒ¨åˆ†ï¼ä½†ç†è§£äº†å®ƒï¼Œä½ å°±ç†è§£äº† LLM ç”Ÿæˆçš„æ ¸å¿ƒä¼˜åŒ–ï¼**

### ğŸ¤” é—®é¢˜ï¼šä¸ºä»€ä¹ˆéœ€è¦ KV Cacheï¼Ÿ

æƒ³è±¡ä½ åœ¨ç”¨ GPT ç”Ÿæˆä¸€å¥è¯ï¼š

```
è¾“å…¥ï¼š"ä»Šå¤©å¤©æ°”"
æœŸæœ›è¾“å‡ºï¼š"ä»Šå¤©å¤©æ°”å¾ˆå¥½ï¼Œé€‚åˆå‡ºå»ç©ã€‚"

ç”Ÿæˆè¿‡ç¨‹ï¼ˆä¸€æ¬¡ç”Ÿæˆä¸€ä¸ªè¯ï¼‰ï¼š
Step 1: "ä»Šå¤©å¤©æ°”" â†’ é¢„æµ‹ â†’ "å¾ˆ"
Step 2: "ä»Šå¤©å¤©æ°”å¾ˆ" â†’ é¢„æµ‹ â†’ "å¥½"
Step 3: "ä»Šå¤©å¤©æ°”å¾ˆå¥½" â†’ é¢„æµ‹ â†’ "ï¼Œ"
Step 4: "ä»Šå¤©å¤©æ°”å¾ˆå¥½ï¼Œ" â†’ é¢„æµ‹ â†’ "é€‚åˆ"
... (æ¯æ¬¡éƒ½è¦é‡æ–°è®¡ç®—æ‰€æœ‰å‰é¢çš„è¯ï¼)
```

### âŒ æ²¡æœ‰ KV Cache çš„é—®é¢˜

æ¯ä¸€æ­¥éƒ½è¦**é‡æ–°è®¡ç®—æ‰€æœ‰å‰é¢çš„è¯**çš„ Key å’Œ Valueï¼š

```python
# Step 1: è¾“å…¥ "ä»Šå¤©å¤©æ°”"
input_1 = ["ä»Šå¤©", "å¤©æ°”"]
K_1 = compute_key(input_1)      # è®¡ç®— ["ä»Šå¤©"çš„K, "å¤©æ°”"çš„K]
V_1 = compute_value(input_1)    # è®¡ç®— ["ä»Šå¤©"çš„V, "å¤©æ°”"çš„V]
Q_1 = compute_query("å¤©æ°”")     # åªéœ€è¦æœ€åä¸€ä¸ªè¯çš„ Q
output_1 = attention(Q_1, K_1, V_1)  # é¢„æµ‹ "å¾ˆ"

# Step 2: è¾“å…¥ "ä»Šå¤©å¤©æ°”å¾ˆ"
input_2 = ["ä»Šå¤©", "å¤©æ°”", "å¾ˆ"]
K_2 = compute_key(input_2)      # åˆè®¡ç®—äº†ä¸€é "ä»Šå¤©" å’Œ "å¤©æ°”" çš„ Kï¼âŒ
V_2 = compute_value(input_2)    # åˆè®¡ç®—äº†ä¸€é "ä»Šå¤©" å’Œ "å¤©æ°”" çš„ Vï¼âŒ
Q_2 = compute_query("å¾ˆ")
output_2 = attention(Q_2, K_2, V_2)  # é¢„æµ‹ "å¥½"

# Step 3: è¾“å…¥ "ä»Šå¤©å¤©æ°”å¾ˆå¥½"
input_3 = ["ä»Šå¤©", "å¤©æ°”", "å¾ˆ", "å¥½"]
K_3 = compute_key(input_3)      # åˆåˆåˆè®¡ç®—äº†ä¸€éå‰é¢æ‰€æœ‰è¯ï¼âŒâŒâŒ
V_3 = compute_value(input_3)    # æµªè´¹è®¡ç®—ï¼
...
```

**é—®é¢˜**ï¼š

- æ¯æ¬¡éƒ½é‡å¤è®¡ç®—å‰é¢çš„ K å’Œ V
- ç”Ÿæˆ 100 ä¸ªè¯ï¼Œå‰é¢çš„è¯ä¼šè¢«é‡å¤è®¡ç®— 99 æ¬¡ï¼
- æµªè´¹å¤§é‡è®¡ç®—èµ„æºå’Œæ—¶é—´

### âœ… æœ‰ KV Cache çš„ä¼˜åŒ–

**æ ¸å¿ƒæ€æƒ³**ï¼šæŠŠå·²ç»è®¡ç®—è¿‡çš„ K å’Œ V **ç¼“å­˜èµ·æ¥**ï¼Œä¸‹æ¬¡ç›´æ¥ç”¨ï¼

```python
# åˆå§‹åŒ–ç©ºç¼“å­˜
cache_K = []
cache_V = []

# Step 1: è¾“å…¥ "ä»Šå¤©å¤©æ°”"
new_K_1 = compute_key(["ä»Šå¤©", "å¤©æ°”"])
new_V_1 = compute_value(["ä»Šå¤©", "å¤©æ°”"])
cache_K = new_K_1  # ä¿å­˜åˆ°ç¼“å­˜
cache_V = new_V_1  # ä¿å­˜åˆ°ç¼“å­˜
Q_1 = compute_query("å¤©æ°”")
output_1 = attention(Q_1, cache_K, cache_V)  # é¢„æµ‹ "å¾ˆ"

# Step 2: è¾“å…¥æ–°è¯ "å¾ˆ"
new_K_2 = compute_key(["å¾ˆ"])         # åªè®¡ç®—æ–°è¯ï¼âœ…
new_V_2 = compute_value(["å¾ˆ"])       # åªè®¡ç®—æ–°è¯ï¼âœ…
cache_K = concat(cache_K, new_K_2)    # æ‹¼æ¥åˆ°ç¼“å­˜
cache_V = concat(cache_V, new_V_2)    # æ‹¼æ¥åˆ°ç¼“å­˜
Q_2 = compute_query("å¾ˆ")
output_2 = attention(Q_2, cache_K, cache_V)  # é¢„æµ‹ "å¥½"

# Step 3: è¾“å…¥æ–°è¯ "å¥½"
new_K_3 = compute_key(["å¥½"])         # åªè®¡ç®—æ–°è¯ï¼âœ…
new_V_3 = compute_value(["å¥½"])       # åªè®¡ç®—æ–°è¯ï¼âœ…
cache_K = concat(cache_K, new_K_3)    # æ‹¼æ¥åˆ°ç¼“å­˜
cache_V = concat(cache_V, new_V_3)    # æ‹¼æ¥åˆ°ç¼“å­˜
...
```

**ä¼˜åŒ–æ•ˆæœ**ï¼š

- âœ… æ¯ä¸ªè¯çš„ K å’Œ V **åªè®¡ç®—ä¸€æ¬¡**
- âœ… ç”Ÿæˆé€Ÿåº¦æå‡ **æ•°åå€**ï¼
- âœ… è¿™å°±æ˜¯ä¸ºä»€ä¹ˆ ChatGPT èƒ½å¿«é€Ÿç”Ÿæˆçš„ç§˜å¯†

### ğŸ“Š æ€§èƒ½å¯¹æ¯”

```
ç”Ÿæˆ 100 ä¸ªè¯çš„è®¡ç®—é‡ï¼š

æ²¡æœ‰ KV Cache:
è®¡ç®—æ¬¡æ•° = 1 + 2 + 3 + ... + 100 = 5050 æ¬¡ K/V è®¡ç®— âŒ

æœ‰ KV Cache:
è®¡ç®—æ¬¡æ•° = 100 æ¬¡ K/V è®¡ç®— âœ…

åŠ é€Ÿæ¯” = 5050 / 100 = 50.5 å€ï¼ğŸš€
```

### ğŸ’» ä»£ç å®ç°

```python
class MultiHeadAttentionWithCache(nn.Module):
    def __init__(self, dim, n_heads):
        super().__init__()
        self.n_heads = n_heads
        self.head_dim = dim // n_heads

        self.wq = nn.Linear(dim, dim, bias=False)
        self.wk = nn.Linear(dim, dim, bias=False)
        self.wv = nn.Linear(dim, dim, bias=False)
        self.wo = nn.Linear(dim, dim, bias=False)

    def forward(self, x, cache=None):
        """
        å‚æ•°:
            x: è¾“å…¥ï¼Œshape = (batch, seq_len, dim)
            cache: ç¼“å­˜çš„ K å’Œ Vï¼Œæ ¼å¼ {'k': tensor, 'v': tensor}

        è¿”å›:
            output: è¾“å‡º
            new_cache: æ›´æ–°åçš„ç¼“å­˜
        """
        batch_size, seq_len, _ = x.shape

        # è®¡ç®—æ–°çš„ Q, K, V
        q = self.wq(x)  # (batch, seq_len, dim)
        k_new = self.wk(x)  # (batch, seq_len, dim)
        v_new = self.wv(x)  # (batch, seq_len, dim)

        # å¦‚æœæœ‰ç¼“å­˜ï¼Œæ‹¼æ¥å†å²çš„ K å’Œ V
        if cache is not None:
            k = torch.cat([cache['k'], k_new], dim=1)  # æ‹¼æ¥å†å²
            v = torch.cat([cache['v'], v_new], dim=1)
        else:
            k = k_new
            v = v_new

        # Reshape for multi-head attention
        q = q.view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)
        k = k.view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)
        v = v.view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)

        # Attention è®¡ç®—
        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)
        attn = F.softmax(scores, dim=-1)
        out = torch.matmul(attn, v)

        # Reshape back
        out = out.transpose(1, 2).contiguous().view(batch_size, seq_len, -1)
        out = self.wo(out)

        # ä¿å­˜å½“å‰çš„ K å’Œ V åˆ°ç¼“å­˜ï¼ˆä¾›ä¸‹æ¬¡ä½¿ç”¨ï¼‰
        new_cache = {'k': k.transpose(1, 2), 'v': v.transpose(1, 2)}

        return out, new_cache


# ä½¿ç”¨ç¤ºä¾‹
model = MultiHeadAttentionWithCache(dim=512, n_heads=8)
cache = None  # åˆå§‹åŒ–ç©ºç¼“å­˜

# æ¨¡æ‹Ÿç”Ÿæˆè¿‡ç¨‹
input_ids = tokenizer.encode("ä»Šå¤©å¤©æ°”")

for step in range(10):  # ç”Ÿæˆ 10 ä¸ªè¯
    # åªè¾“å…¥æ–°è¯ï¼ˆç¬¬ä¸€æ¬¡è¾“å…¥æ‰€æœ‰è¯ï¼‰
    if step == 0:
        x = embed(input_ids)  # (1, 2, 512)
    else:
        x = embed([new_token_id])  # (1, 1, 512) åªè¾“å…¥æ–°è¯ï¼

    # å‰å‘ä¼ æ’­ï¼ˆè‡ªåŠ¨ä½¿ç”¨ç¼“å­˜ï¼‰
    output, cache = model(x, cache=cache)  # cache ä¼šè‡ªåŠ¨ç´¯ç§¯

    # é¢„æµ‹ä¸‹ä¸€ä¸ªè¯
    new_token_id = output.argmax(dim=-1)
    input_ids.append(new_token_id)
```

### ğŸ¨ å›¾è§£ KV Cache

```
æ—¶åˆ» t=1: è¾“å…¥ "ä»Šå¤©"
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Input: "ä»Šå¤©"                        â”‚
â”‚ Compute: Q1, K1, V1                 â”‚
â”‚ Cache: K1, V1                       â”‚ â† ä¿å­˜åˆ°ç¼“å­˜
â”‚ Attention(Q1, K1, V1) â†’ predict     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æ—¶åˆ» t=2: è¾“å…¥ "å¤©æ°”" (æ–°è¯)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Input: "å¤©æ°”"                        â”‚
â”‚ Compute: Q2, K2, V2 (åªè®¡ç®—æ–°è¯!)    â”‚
â”‚ Cache: [K1, K2], [V1, V2]           â”‚ â† æ‹¼æ¥åˆ°ç¼“å­˜
â”‚ Attention(Q2, [K1,K2], [V1,V2])     â”‚ â† ç”¨å®Œæ•´ç¼“å­˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æ—¶åˆ» t=3: è¾“å…¥ "å¾ˆ" (æ–°è¯)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Input: "å¾ˆ"                          â”‚
â”‚ Compute: Q3, K3, V3 (åªè®¡ç®—æ–°è¯!)    â”‚
â”‚ Cache: [K1,K2,K3], [V1,V2,V3]       â”‚ â† ç»§ç»­æ‹¼æ¥
â”‚ Attention(Q3, [K1,K2,K3], [V1,V2,V3])â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## æ ¸å¿ƒæ¦‚å¿µ 3ï¼šRoPE ä½ç½®ç¼–ç 

### ğŸ¯ ä¸ºä»€ä¹ˆéœ€è¦ä½ç½®ç¼–ç ï¼Ÿ

å›å¿†ä¸€ä¸‹ï¼šSelf-Attention æœ¬èº«**æ²¡æœ‰ä½ç½®ä¿¡æ¯**ï¼

```
å¥å­ A: "æˆ‘ çˆ± ä½ "
å¥å­ B: "ä½  çˆ± æˆ‘"

å¦‚æœæ²¡æœ‰ä½ç½®ç¼–ç ï¼ŒSelf-Attention ä¼šè®¤ä¸ºå®ƒä»¬ä¸€æ ·ï¼
å› ä¸ºåŒ…å«çš„è¯ç›¸åŒï¼Œåªæ˜¯é¡ºåºä¸åŒã€‚
```

### ğŸ“ ä½ ç¬¬äºŒç« å­¦è¿‡çš„ä½ç½®ç¼–ç ï¼ˆSinusoidalï¼‰

```python
# åŸå§‹ Transformer çš„ä½ç½®ç¼–ç ï¼ˆåŠ æ³•ï¼‰
def get_positional_encoding(seq_len, d_model):
    position = torch.arange(seq_len).unsqueeze(1)
    div_term = torch.exp(torch.arange(0, d_model, 2) *
                        -(math.log(10000.0) / d_model))

    pe = torch.zeros(seq_len, d_model)
    pe[:, 0::2] = torch.sin(position * div_term)
    pe[:, 1::2] = torch.cos(position * div_term)
    return pe

# ä½¿ç”¨æ–¹å¼ï¼šç›´æ¥åŠ åˆ° embedding ä¸Š
x = token_embedding + positional_encoding
```

**é—®é¢˜**ï¼š

- ä½ç½®ä¿¡æ¯å’Œå†…å®¹ä¿¡æ¯"æ··åœ¨ä¸€èµ·"äº†
- éš¾ä»¥æ³›åŒ–åˆ°æ›´é•¿çš„åºåˆ—

### ğŸ”„ RoPE (Rotary Position Embedding)

**æ ¸å¿ƒæ€æƒ³**ï¼šé€šè¿‡**æ—‹è½¬å‘é‡**æ¥ç¼–ç ä½ç½®ä¿¡æ¯

```
ä¼ ç»Ÿæ–¹å¼ï¼šx + pos_encoding        ï¼ˆåŠ æ³•ï¼‰
RoPEæ–¹å¼ï¼šrotate(x, Î¸)            ï¼ˆæ—‹è½¬ï¼‰

å…¶ä¸­æ—‹è½¬è§’åº¦ Î¸ å–å†³äºä½ç½®ï¼š
ä½ç½® 0 â†’ æ—‹è½¬ 0Â°
ä½ç½® 1 â†’ æ—‹è½¬ Î¸
ä½ç½® 2 â†’ æ—‹è½¬ 2Î¸
ä½ç½® 3 â†’ æ—‹è½¬ 3Î¸
...
```

### ğŸ¨ ç›´è§‚ç†è§£

æƒ³è±¡æ¯ä¸ªè¯å‘é‡æ˜¯äºŒç»´å¹³é¢ä¸Šçš„ç®­å¤´ï¼š

```
ä½ç½®ç¼–ç  = æ—‹è½¬ç®­å¤´

åŸå§‹å‘é‡ "ä»Šå¤©" at ä½ç½® 0:
  â†’  (ä¸æ—‹è½¬)

"ä»Šå¤©" at ä½ç½® 1:
  â†—  (æ—‹è½¬ 30Â°)

"ä»Šå¤©" at ä½ç½® 2:
  â†‘  (æ—‹è½¬ 60Â°)

"ä»Šå¤©" at ä½ç½® 3:
  â†–  (æ—‹è½¬ 90Â°)
```

**å…³é”®æ€§è´¨**ï¼šä¸¤ä¸ªå‘é‡çš„**ç›¸å¯¹ä½ç½®**å¯ä»¥é€šè¿‡**ç›¸å¯¹æ—‹è½¬è§’åº¦**è¡¨ç¤ºï¼

```
è¯ A åœ¨ä½ç½® 1 (æ—‹è½¬ 30Â°)
è¯ B åœ¨ä½ç½® 3 (æ—‹è½¬ 90Â°)
ç›¸å¯¹è§’åº¦ = 90Â° - 30Â° = 60Â°

æ— è®º A å’Œ B åœ¨å“ªä¸ªä½ç½®ï¼Œåªè¦ç›¸å¯¹è·ç¦»æ˜¯ 2ï¼Œ
ç›¸å¯¹è§’åº¦æ€»æ˜¯ 60Â°ï¼

è¿™è®©æ¨¡å‹æ›´å®¹æ˜“å­¦ä¹ ç›¸å¯¹ä½ç½®å…³ç³»ï¼
```

### ğŸ’» RoPE ç®€åŒ–å®ç°

```python
def precompute_freqs_cis(dim, max_seq_len, theta=10000.0):
    """
    é¢„è®¡ç®—æ—‹è½¬é¢‘ç‡

    å‚æ•°:
        dim: å‘é‡ç»´åº¦ï¼ˆé€šå¸¸æ˜¯ head_dimï¼‰
        max_seq_len: æœ€å¤§åºåˆ—é•¿åº¦
        theta: åŸºç¡€é¢‘ç‡ï¼ˆè¶Šå¤§ï¼Œæ—‹è½¬è¶Šæ…¢ï¼‰
    """
    # è®¡ç®—æ¯ä¸ªç»´åº¦çš„é¢‘ç‡
    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))
    # freqs shape: (dim/2,)

    # è®¡ç®—æ¯ä¸ªä½ç½®çš„è§’åº¦
    t = torch.arange(max_seq_len)  # [0, 1, 2, ..., max_seq_len-1]
    # t shape: (max_seq_len,)

    # å¤–ç§¯ï¼šä½ç½® Ã— é¢‘ç‡ = è§’åº¦
    freqs = torch.outer(t, freqs).float()
    # freqs shape: (max_seq_len, dim/2)

    # è½¬æ¢ä¸ºå¤æ•°å½¢å¼ï¼ˆç”¨äºæ—‹è½¬ï¼‰
    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)
    # freqs_cis shape: (max_seq_len, dim/2)

    return freqs_cis


def apply_rotary_emb(x, freqs_cis):
    """
    åº”ç”¨æ—‹è½¬ä½ç½®ç¼–ç 

    å‚æ•°:
        x: è¾“å…¥å‘é‡ï¼Œshape = (..., seq_len, dim)
        freqs_cis: é¢„è®¡ç®—çš„æ—‹è½¬é¢‘ç‡

    è¿”å›:
        æ—‹è½¬åçš„å‘é‡
    """
    # å°†å®æ•°å‘é‡è½¬æ¢ä¸ºå¤æ•°ï¼ˆæ¯ä¸¤ä¸ªç»´åº¦ä¸€ç»„ï¼‰
    x_complex = torch.view_as_complex(
        x.float().reshape(*x.shape[:-1], -1, 2)
    )
    # x_complex shape: (..., seq_len, dim/2)

    # åº”ç”¨æ—‹è½¬ï¼ˆå¤æ•°ä¹˜æ³• = æ—‹è½¬ï¼‰
    x_rotated = x_complex * freqs_cis

    # è½¬æ¢å›å®æ•°
    x_out = torch.view_as_real(x_rotated).flatten(-2)

    return x_out.type_as(x)


# åœ¨ Attention ä¸­ä½¿ç”¨
class AttentionWithRoPE(nn.Module):
    def __init__(self, dim, n_heads, max_seq_len):
        super().__init__()
        self.head_dim = dim // n_heads

        # é¢„è®¡ç®—æ—‹è½¬é¢‘ç‡
        self.freqs_cis = precompute_freqs_cis(self.head_dim, max_seq_len)

    def forward(self, q, k, v, start_pos=0):
        # è·å–å½“å‰åºåˆ—çš„æ—‹è½¬é¢‘ç‡
        seq_len = q.size(1)
        freqs_cis = self.freqs_cis[start_pos:start_pos + seq_len]

        # åªå¯¹ Q å’Œ K åº”ç”¨ RoPEï¼ˆV ä¸éœ€è¦ï¼‰
        q = apply_rotary_emb(q, freqs_cis)
        k = apply_rotary_emb(k, freqs_cis)

        # æ­£å¸¸çš„ Attention è®¡ç®—
        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)
        attn = F.softmax(scores, dim=-1)
        out = torch.matmul(attn, v)

        return out
```

### ğŸ“Š RoPE vs ä¼ ç»Ÿä½ç½®ç¼–ç 

| ç‰¹æ€§         | ä¼ ç»Ÿ Sinusoidal  | RoPE                   |
| ------------ | ---------------- | ---------------------- |
| **ç¼–ç æ–¹å¼** | åŠ æ³• (x + PE)    | æ—‹è½¬ (rotate)          |
| **ä½ç½®ä¿¡æ¯** | ç»å¯¹ä½ç½®         | ç›¸å¯¹ä½ç½®               |
| **æ³›åŒ–èƒ½åŠ›** | è¾ƒå·®             | æ›´å¥½                   |
| **é•¿åº¦å¤–æ¨** | å›°éš¾             | å®¹æ˜“                   |
| **ä½¿ç”¨æ¨¡å‹** | åŸå§‹ Transformer | GPT-NeoX, LLaMA, GPT-J |

---

## æ ¸å¿ƒæ¦‚å¿µ 4ï¼šSwiGLU æ¿€æ´»å‡½æ•°

### ğŸ“ ä½ ç¬¬äºŒç« å­¦è¿‡çš„ FFNï¼ˆå‰é¦ˆç¥ç»ç½‘ç»œï¼‰

```python
class MLP(nn.Module):
    def __init__(self, dim, hidden_dim):
        super().__init__()
        self.w1 = nn.Linear(dim, hidden_dim)
        self.w2 = nn.Linear(hidden_dim, dim)

    def forward(self, x):
        # ç®€å•çš„ä¸¤å±‚ç»“æ„
        return self.w2(F.relu(self.w1(x)))
        #               â†‘ ReLU æ¿€æ´»å‡½æ•°
```

**é—®é¢˜**ï¼š

- ReLU åœ¨è´Ÿæ•°åŒºåŸŸæ¢¯åº¦ä¸º 0ï¼ˆæ­»äº¡ ReLU é—®é¢˜ï¼‰
- å¯èƒ½é™åˆ¶æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›

### ğŸ”„ æ¿€æ´»å‡½æ•°çš„æ¼”åŒ–

```
1. ReLU (2012)
   f(x) = max(0, x)

   ä¼˜ç‚¹ï¼šç®€å•ï¼Œè®¡ç®—å¿«
   ç¼ºç‚¹ï¼šè´Ÿæ•°æ¢¯åº¦ä¸º 0

2. GELU (2016) [BERT ä½¿ç”¨]
   f(x) = x Â· Î¦(x)  (Î¦ æ˜¯æ ‡å‡†æ­£æ€åˆ†å¸ƒçš„CDF)

   ä¼˜ç‚¹ï¼šæ›´å¹³æ»‘ï¼Œæ€§èƒ½æ›´å¥½
   ç¼ºç‚¹ï¼šè®¡ç®—ç¨æ…¢

3. Swish / SiLU (2017) [æ¥è¿‘ GELU]
   f(x) = x Â· sigmoid(x)

   ä¼˜ç‚¹ï¼šç®€å•ï¼Œæ€§èƒ½å¥½

4. GLU (Gated Linear Unit, 2017)
   f(x) = x âŠ™ sigmoid(Wx)

   ä¼˜ç‚¹ï¼šå¼•å…¥é—¨æ§æœºåˆ¶
   ç¼ºç‚¹ï¼šéœ€è¦é¢å¤–çš„å‚æ•°çŸ©é˜µ

5. SwiGLU (2020) [LLaMA, PaLM ä½¿ç”¨]
   f(x) = Swish(W1Â·x) âŠ™ (W3Â·x)

   ä¼˜ç‚¹ï¼šç»“åˆ Swish å’Œ GLU çš„ä¼˜ç‚¹
```

### ğŸ’» SwiGLU å®ç°

```python
class SwiGLU_FFN(nn.Module):
    """
    SwiGLU = Swish(W1Â·x) âŠ™ W3Â·x

    ç›¸æ¯”ä¼ ç»Ÿ FFNï¼Œå¤šäº†ä¸€ä¸ªçº¿æ€§å±‚ W3
    """
    def __init__(self, dim, hidden_dim, dropout=0.0):
        super().__init__()

        # ä¸‰ä¸ªçº¿æ€§å±‚ï¼ˆä¼ ç»Ÿ FFN åªæœ‰ä¸¤ä¸ªï¼‰
        self.w1 = nn.Linear(dim, hidden_dim, bias=False)      # Gate
        self.w2 = nn.Linear(hidden_dim, dim, bias=False)      # Down projection
        self.w3 = nn.Linear(dim, hidden_dim, bias=False)      # Up projection

        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        # SwiGLU è®¡ç®—
        # 1. é€šè¿‡ W1 è®¡ç®—é—¨æ§ä¿¡å·
        gate = F.silu(self.w1(x))  # Swish/SiLU æ¿€æ´»

        # 2. é€šè¿‡ W3 è®¡ç®—ç‰¹å¾
        features = self.w3(x)

        # 3. é—¨æ§ï¼šé€å…ƒç´ ç›¸ä¹˜
        hidden = gate * features

        # 4. æŠ•å½±å›åŸå§‹ç»´åº¦
        output = self.w2(hidden)
        output = self.dropout(output)

        return output


# å¯¹æ¯”ï¼šä¼ ç»Ÿ FFN
class Traditional_FFN(nn.Module):
    def __init__(self, dim, hidden_dim, dropout=0.0):
        super().__init__()
        self.w1 = nn.Linear(dim, hidden_dim, bias=False)
        self.w2 = nn.Linear(hidden_dim, dim, bias=False)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        hidden = F.relu(self.w1(x))  # ReLU æ¿€æ´»
        output = self.w2(hidden)
        output = self.dropout(output)
        return output
```

### ğŸ¨ å¯è§†åŒ–å¯¹æ¯”

```
ä¼ ç»Ÿ FFN (ReLU):
Input (dim)
    â†“
  W1 (Linear)
    â†“
  ReLU
    â†“
  W2 (Linear)
    â†“
Output (dim)

å‚æ•°é‡: W1 + W2


SwiGLU FFN:
Input (dim)
    â†“
    â”œâ”€â”€â†’ W1 â†’ Swish â”€â”€â”
    â”‚                  â†“
    â””â”€â”€â†’ W3 â”€â”€â”€â”€â”€â”€â†’  âŠ™  (é€å…ƒç´ ç›¸ä¹˜)
                      â†“
                     W2
                      â†“
                  Output (dim)

å‚æ•°é‡: W1 + W2 + W3  (å¤šäº† 33%ï¼Œä½†æ€§èƒ½æå‡æ›´å¤šï¼)
```

### ğŸ“Š æ€§èƒ½å¯¹æ¯”ï¼ˆå®éªŒç»“æœï¼‰

åœ¨ LLaMA è®ºæ–‡ä¸­çš„å®éªŒï¼š

| æ¿€æ´»å‡½æ•°   | PPL (è¶Šä½è¶Šå¥½) | å‚æ•°é‡ |
| ---------- | -------------- | ------ |
| ReLU       | 9.8            | 1.0x   |
| GELU       | 9.5            | 1.0x   |
| **SwiGLU** | **9.2** âœ…     | 1.33x  |

**ç»“è®º**ï¼šå¤š 33% å‚æ•°ï¼Œä½†æ€§èƒ½æå‡æ˜¾è‘—ï¼Œæ€§ä»·æ¯”é«˜ï¼

---

## æ ¸å¿ƒæ¦‚å¿µ 5ï¼šRMS Norm

### ğŸ“ ä½ ç¬¬äºŒç« å­¦è¿‡çš„ Layer Norm

```python
class LayerNorm(nn.Module):
    def __init__(self, features, eps=1e-6):
        super().__init__()
        self.a = nn.Parameter(torch.ones(features))
        self.b = nn.Parameter(torch.zeros(features))
        self.eps = eps

    def forward(self, x):
        # è®¡ç®—å‡å€¼å’Œæ ‡å‡†å·®
        mean = x.mean(-1, keepdim=True)
        std = x.std(-1, keepdim=True)

        # æ ‡å‡†åŒ–
        x_norm = (x - mean) / (std + self.eps)

        # ç¼©æ”¾å’Œå¹³ç§»
        return self.a * x_norm + self.b
```

**Layer Norm åšäº†ä»€ä¹ˆï¼Ÿ**

1. **ä¸­å¿ƒåŒ–**ï¼šå‡å»å‡å€¼ (x - mean)
2. **æ ‡å‡†åŒ–**ï¼šé™¤ä»¥æ ‡å‡†å·® (/ std)
3. **ç¼©æ”¾å’Œå¹³ç§»**ï¼šå¯å­¦ä¹ å‚æ•° a å’Œ b

### ğŸ”„ RMS Norm (Root Mean Square Norm)

**æ ¸å¿ƒæ€æƒ³**ï¼šåªåšæ ‡å‡†åŒ–ï¼Œä¸åšä¸­å¿ƒåŒ–ï¼

```python
class RMSNorm(nn.Module):
    def __init__(self, dim, eps=1e-6):
        super().__init__()
        self.weight = nn.Parameter(torch.ones(dim))
        self.eps = eps

    def forward(self, x):
        # è®¡ç®— RMS (å‡æ–¹æ ¹)
        rms = torch.sqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)

        # æ ‡å‡†åŒ–ï¼ˆä¸å‡å‡å€¼ï¼ï¼‰
        x_norm = x / rms

        # åªç¼©æ”¾ï¼Œä¸å¹³ç§»
        return self.weight * x_norm
```

### ğŸ“Š å…¬å¼å¯¹æ¯”

**Layer Norm**:

```
LN(x) = Î³ Â· (x - Î¼) / Ïƒ + Î²

å…¶ä¸­:
Î¼ = mean(x)         # å‡å€¼
Ïƒ = std(x)          # æ ‡å‡†å·®
Î³, Î² æ˜¯å¯å­¦ä¹ å‚æ•°
```

**RMS Norm**:

```
RMS(x) = Î³ Â· x / RMS(x)

å…¶ä¸­:
RMS(x) = sqrt(mean(xÂ²))  # å‡æ–¹æ ¹
Î³ æ˜¯å¯å­¦ä¹ å‚æ•°ï¼ˆæ²¡æœ‰ Î²ï¼ï¼‰
```

### âš¡ ä¸ºä»€ä¹ˆ RMS Norm æ›´å¥½ï¼Ÿ

| ç‰¹æ€§           | Layer Norm              | RMS Norm             |
| -------------- | ----------------------- | -------------------- |
| **è®¡ç®—å¤æ‚åº¦** | é«˜ï¼ˆéœ€è¦ç®— mean + stdï¼‰ | ä½ï¼ˆåªéœ€è¦ç®— RMSï¼‰   |
| **å‚æ•°æ•°é‡**   | 2 Ã— dim (Î³ å’Œ Î²)        | 1 Ã— dim (åªæœ‰ Î³)     |
| **è®­ç»ƒé€Ÿåº¦**   | æ…¢                      | å¿« 5-10% âš¡          |
| **æ•ˆæœ**       | å¥½                      | å‡ ä¹ä¸€æ ·å¥½           |
| **ç¨³å®šæ€§**     | å¥½                      | æ›´å¥½ï¼ˆä¸éœ€è¦ä¸­å¿ƒåŒ–ï¼‰ |

**å…³é”®æ´å¯Ÿ**ï¼š

- åœ¨ Transformer ä¸­ï¼Œ**ä¸­å¿ƒåŒ–ä¸æ˜¯å¿…éœ€çš„**ï¼
- å»æ‰ mean è®¡ç®—å¯ä»¥åŠ é€Ÿï¼Œä¸”ä¸å½±å“æ€§èƒ½
- å¤§æ¨¡å‹è®­ç»ƒæ—¶ï¼Œæ¯ä¸€ç‚¹åŠ é€Ÿéƒ½å¾ˆé‡è¦

### ğŸ’» å®Œæ•´å¯¹æ¯”ä»£ç 

```python
import torch
import torch.nn as nn
import time

# 1. Layer Norm
class LayerNorm(nn.Module):
    def __init__(self, dim, eps=1e-6):
        super().__init__()
        self.weight = nn.Parameter(torch.ones(dim))
        self.bias = nn.Parameter(torch.zeros(dim))
        self.eps = eps

    def forward(self, x):
        mean = x.mean(-1, keepdim=True)
        std = x.std(-1, keepdim=True)
        return self.weight * (x - mean) / (std + self.eps) + self.bias

# 2. RMS Norm
class RMSNorm(nn.Module):
    def __init__(self, dim, eps=1e-6):
        super().__init__()
        self.weight = nn.Parameter(torch.ones(dim))
        self.eps = eps

    def forward(self, x):
        rms = torch.sqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        return self.weight * x / rms

# æ€§èƒ½æµ‹è¯•
def benchmark():
    dim = 4096
    batch_size = 32
    seq_len = 2048

    x = torch.randn(batch_size, seq_len, dim).cuda()

    ln = LayerNorm(dim).cuda()
    rms = RMSNorm(dim).cuda()

    # é¢„çƒ­
    for _ in range(10):
        _ = ln(x)
        _ = rms(x)

    # Layer Norm æµ‹é€Ÿ
    torch.cuda.synchronize()
    start = time.time()
    for _ in range(100):
        _ = ln(x)
    torch.cuda.synchronize()
    ln_time = time.time() - start

    # RMS Norm æµ‹é€Ÿ
    torch.cuda.synchronize()
    start = time.time()
    for _ in range(100):
        _ = rms(x)
    torch.cuda.synchronize()
    rms_time = time.time() - start

    print(f"Layer Norm: {ln_time:.4f}s")
    print(f"RMS Norm:   {rms_time:.4f}s")
    print(f"Speedup:    {ln_time/rms_time:.2f}x")

# è¿è¡Œæµ‹è¯•
# benchmark()
# è¾“å‡ºç¤ºä¾‹:
# Layer Norm: 0.1234s
# RMS Norm:   0.1089s
# Speedup:    1.13x
```

---

## å®Œæ•´ä»£ç ç¤ºä¾‹

### ğŸ¯ æ„å»ºä¸€ä¸ªç®€åŒ–ç‰ˆ GPT Block

æŠŠä¸Šé¢æ‰€æœ‰æ¦‚å¿µæ•´åˆåˆ°ä¸€èµ·ï¼š

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import math


class RMSNorm(nn.Module):
    """RMS Normalization"""
    def __init__(self, dim, eps=1e-6):
        super().__init__()
        self.weight = nn.Parameter(torch.ones(dim))
        self.eps = eps

    def forward(self, x):
        rms = torch.sqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        return self.weight * x / rms


def precompute_freqs_cis(dim, max_seq_len, theta=10000.0):
    """é¢„è®¡ç®— RoPE çš„æ—‹è½¬é¢‘ç‡"""
    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))
    t = torch.arange(max_seq_len)
    freqs = torch.outer(t, freqs).float()
    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)
    return freqs_cis


def apply_rotary_emb(x, freqs_cis):
    """åº”ç”¨ RoPE"""
    x_complex = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2))
    freqs_cis = freqs_cis.unsqueeze(0).unsqueeze(0)  # (1, 1, seq_len, dim/2)
    x_rotated = x_complex * freqs_cis
    x_out = torch.view_as_real(x_rotated).flatten(-2)
    return x_out.type_as(x)


class MultiHeadAttentionWithCache(nn.Module):
    """å¸¦ KV Cache å’Œ RoPE çš„å¤šå¤´æ³¨æ„åŠ›"""
    def __init__(self, dim, n_heads, max_seq_len):
        super().__init__()
        self.n_heads = n_heads
        self.head_dim = dim // n_heads
        self.dim = dim

        self.wq = nn.Linear(dim, dim, bias=False)
        self.wk = nn.Linear(dim, dim, bias=False)
        self.wv = nn.Linear(dim, dim, bias=False)
        self.wo = nn.Linear(dim, dim, bias=False)

        # é¢„è®¡ç®— RoPE é¢‘ç‡
        self.freqs_cis = precompute_freqs_cis(self.head_dim, max_seq_len)

        # æ³¨å†Œ causal mask
        mask = torch.full((1, 1, max_seq_len, max_seq_len), float("-inf"))
        mask = torch.triu(mask, diagonal=1)
        self.register_buffer("mask", mask)

    def forward(self, x, cache=None, start_pos=0):
        """
        å‚æ•°:
            x: è¾“å…¥ï¼Œshape = (batch, seq_len, dim)
            cache: KV ç¼“å­˜
            start_pos: å½“å‰ä½ç½®ï¼ˆç”¨äº RoPE å’Œ maskï¼‰
        """
        batch_size, seq_len, _ = x.shape

        # è®¡ç®— Q, K, V
        q = self.wq(x)
        k = self.wk(x)
        v = self.wv(x)

        # Reshape for multi-head
        q = q.view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)
        k = k.view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)
        v = v.view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)

        # åº”ç”¨ RoPE
        freqs_cis = self.freqs_cis[start_pos:start_pos + seq_len].to(x.device)
        q = apply_rotary_emb(q, freqs_cis)
        k = apply_rotary_emb(k, freqs_cis)

        # KV Cache
        if cache is not None:
            k = torch.cat([cache['k'], k], dim=2)
            v = torch.cat([cache['v'], v], dim=2)

        # Attention
        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)

        # åº”ç”¨ causal mask
        total_len = k.size(2)
        scores = scores + self.mask[:, :, start_pos:start_pos+seq_len, :total_len]

        attn = F.softmax(scores, dim=-1)
        out = torch.matmul(attn, v)

        # Reshape back
        out = out.transpose(1, 2).contiguous().view(batch_size, seq_len, self.dim)
        out = self.wo(out)

        # æ›´æ–°ç¼“å­˜
        new_cache = {'k': k, 'v': v}

        return out, new_cache


class SwiGLU_FFN(nn.Module):
    """SwiGLU å‰é¦ˆç½‘ç»œ"""
    def __init__(self, dim, hidden_dim):
        super().__init__()
        self.w1 = nn.Linear(dim, hidden_dim, bias=False)
        self.w2 = nn.Linear(hidden_dim, dim, bias=False)
        self.w3 = nn.Linear(dim, hidden_dim, bias=False)

    def forward(self, x):
        return self.w2(F.silu(self.w1(x)) * self.w3(x))


class TransformerBlock(nn.Module):
    """å®Œæ•´çš„ Transformer Block (GPT é£æ ¼)"""
    def __init__(self, dim, n_heads, hidden_dim, max_seq_len):
        super().__init__()

        # Attention éƒ¨åˆ†
        self.attention = MultiHeadAttentionWithCache(dim, n_heads, max_seq_len)
        self.attention_norm = RMSNorm(dim)

        # FFN éƒ¨åˆ†
        self.ffn = SwiGLU_FFN(dim, hidden_dim)
        self.ffn_norm = RMSNorm(dim)

    def forward(self, x, cache=None, start_pos=0):
        # Attention + Residual
        h, new_cache = self.attention(
            self.attention_norm(x),
            cache=cache,
            start_pos=start_pos
        )
        x = x + h

        # FFN + Residual
        x = x + self.ffn(self.ffn_norm(x))

        return x, new_cache


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # é…ç½®
    dim = 512
    n_heads = 8
    hidden_dim = 2048
    max_seq_len = 2048

    # åˆ›å»ºæ¨¡å‹
    block = TransformerBlock(dim, n_heads, hidden_dim, max_seq_len)

    # æ¨¡æ‹Ÿç”Ÿæˆè¿‡ç¨‹
    batch_size = 1
    vocab_size = 50000

    # å‡è®¾å·²æœ‰ embedding å±‚
    embedding = nn.Embedding(vocab_size, dim)

    # åˆå§‹è¾“å…¥ï¼š"ä»Šå¤©å¤©æ°”" (å‡è®¾ token ids = [1234, 5678])
    input_ids = torch.tensor([[1234, 5678]])
    x = embedding(input_ids)  # (1, 2, 512)

    cache = None
    start_pos = 0

    # ç¬¬ä¸€æ¬¡å‰å‘ï¼ˆå¤„ç†åˆå§‹è¾“å…¥ï¼‰
    print("Step 1: å¤„ç† 'ä»Šå¤©å¤©æ°”'")
    x, cache = block(x, cache=cache, start_pos=start_pos)
    print(f"Output shape: {x.shape}")
    print(f"Cache K shape: {cache['k'].shape}")
    start_pos += x.size(1)

    # åç»­ç”Ÿæˆï¼ˆæ¯æ¬¡åªè¾“å…¥ä¸€ä¸ªæ–° tokenï¼‰
    for step in range(5):
        print(f"\nStep {step+2}: ç”Ÿæˆæ–°è¯")

        # æ¨¡æ‹Ÿé¢„æµ‹çš„æ–° token
        new_token_id = torch.randint(0, vocab_size, (1, 1))
        x = embedding(new_token_id)  # (1, 1, 512) åªè¾“å…¥ä¸€ä¸ªæ–°è¯ï¼

        # å‰å‘ä¼ æ’­ï¼ˆå¤ç”¨ç¼“å­˜ï¼‰
        x, cache = block(x, cache=cache, start_pos=start_pos)
        print(f"Output shape: {x.shape}")
        print(f"Cache K shape: {cache['k'].shape}")  # ç¼“å­˜åœ¨å¢é•¿ï¼

        start_pos += 1

    print("\nâœ… å®Œæ•´çš„ç”Ÿæˆæµç¨‹æ¼”ç¤ºå®Œæˆï¼")
```

### ğŸ“Š è¾“å‡ºç¤ºä¾‹

```
Step 1: å¤„ç† 'ä»Šå¤©å¤©æ°”'
Output shape: torch.Size([1, 2, 512])
Cache K shape: torch.Size([1, 8, 2, 64])

Step 2: ç”Ÿæˆæ–°è¯
Output shape: torch.Size([1, 1, 512])
Cache K shape: torch.Size([1, 8, 3, 64])  â† ç¼“å­˜å¢é•¿äº†ï¼

Step 3: ç”Ÿæˆæ–°è¯
Output shape: torch.Size([1, 1, 512])
Cache K shape: torch.Size([1, 8, 4, 64])  â† ç»§ç»­å¢é•¿

Step 4: ç”Ÿæˆæ–°è¯
Output shape: torch.Size([1, 1, 512])
Cache K shape: torch.Size([1, 8, 5, 64])

Step 5: ç”Ÿæˆæ–°è¯
Output shape: torch.Size([1, 1, 512])
Cache K shape: torch.Size([1, 8, 6, 64])

Step 6: ç”Ÿæˆæ–°è¯
Output shape: torch.Size([1, 1, 512])
Cache K shape: torch.Size([1, 8, 7, 64])

âœ… å®Œæ•´çš„ç”Ÿæˆæµç¨‹æ¼”ç¤ºå®Œæˆï¼
```

---

## å¸¸è§é—®é¢˜ FAQ

### Q1: ä¸ºä»€ä¹ˆ RoPE åªåº”ç”¨åœ¨ Q å’Œ K ä¸Šï¼Œä¸åº”ç”¨åœ¨ V ä¸Šï¼Ÿ

**A**: RoPE çš„ç›®çš„æ˜¯ç¼–ç **ä½ç½®å…³ç³»**ï¼Œè®©æ¨¡å‹çŸ¥é“"è¯ A å’Œè¯ B ä¹‹é—´çš„è·ç¦»"ã€‚

- **Q å’Œ K** ç”¨äºè®¡ç®—æ³¨æ„åŠ›æƒé‡ï¼ˆè°å’Œè°ç›¸å…³ï¼‰
  - éœ€è¦ä½ç½®ä¿¡æ¯ï¼Œå› ä¸ºç›¸å¯¹ä½ç½®å½±å“ç›¸å…³æ€§
- **V** æ˜¯å®é™…çš„å†…å®¹å€¼
  - ä¸éœ€è¦ä½ç½®ä¿¡æ¯ï¼Œå†…å®¹æœ¬èº«ä¸å› ä½ç½®æ”¹å˜

ç±»æ¯”ï¼š

```
Q: "è¯·ç»™æˆ‘é™„è¿‘çš„å’–å•¡åº—"
K: [æ˜Ÿå·´å…‹(500ç±³), å’–å•¡å…A(2å…¬é‡Œ), å’–å•¡å…B(100ç±³)]
   â†‘ éœ€è¦çŸ¥é“è·ç¦»ï¼ˆä½ç½®ï¼‰
V: [æ˜Ÿå·´å…‹çš„èœå•, å’–å•¡å…Açš„èœå•, å’–å•¡å…Bçš„èœå•]
   â†‘ èœå•å†…å®¹ä¸éšè·ç¦»æ”¹å˜
```

### Q2: KV Cache ä¼šä¸ä¼šå ç”¨å¤ªå¤šå†…å­˜ï¼Ÿ

**A**: ä¼šçš„ï¼è¿™æ˜¯ LLM æ¨ç†çš„ä¸»è¦ç“¶é¢ˆä¹‹ä¸€ã€‚

**å†…å­˜å ç”¨è®¡ç®—**ï¼š

```
æ¯ä¸ª token çš„ KV ç¼“å­˜å¤§å° = 2 Ã— n_layers Ã— dim Ã— 2 bytes (FP16)

å‡è®¾ LLaMA-7B:
- n_layers = 32
- dim = 4096
- ç”Ÿæˆ 2048 ä¸ª tokens

KV Cache = 2 Ã— 32 Ã— 4096 Ã— 2048 Ã— 2 bytes
         â‰ˆ 1 GB

ç”Ÿæˆè¶Šé•¿ï¼Œå ç”¨è¶Šå¤§ï¼
```

**ä¼˜åŒ–æ–¹æ³•**ï¼š

- **Grouped Query Attention (GQA)**ï¼šå‡å°‘ K å’Œ V çš„å¤´æ•°
- **Multi-Query Attention (MQA)**ï¼šæ‰€æœ‰å¤´å…±äº«ä¸€ä¸ª K å’Œ V
- **PagedAttention**ï¼šåˆ†é¡µç®¡ç†ç¼“å­˜ï¼ˆvLLM ä½¿ç”¨ï¼‰

### Q3: ä¸ºä»€ä¹ˆå¤§æ¨¡å‹éƒ½ç”¨ RMSNorm è€Œä¸æ˜¯ LayerNormï¼Ÿ

**A**: ä¸»è¦æ˜¯**é€Ÿåº¦**ï¼

åœ¨å¤§æ¨¡å‹ä¸­ï¼š

- è®­ç»ƒæˆæœ¬ = æ•°åƒä¸‡ç¾å…ƒ
- åŠ é€Ÿ 5% = èŠ‚çœæ•°ç™¾ä¸‡ç¾å…ƒ
- RMSNorm å‡ ä¹ä¸å½±å“æ•ˆæœï¼Œä½†èƒ½åŠ é€Ÿ 5-10%

æ€§ä»·æ¯”æé«˜ï¼

### Q4: SwiGLU æ¯” ReLU å¥½å¤šå°‘ï¼Ÿ

**A**: æ ¹æ®å®éªŒï¼ˆLLaMA, PaLM è®ºæ–‡ï¼‰ï¼š

- åœ¨å°æ¨¡å‹ï¼ˆ<1Bï¼‰ï¼šæå‡ä¸æ˜æ˜¾
- åœ¨å¤§æ¨¡å‹ï¼ˆ>10Bï¼‰ï¼šæå‡æ˜¾è‘—ï¼ˆPPL é™ä½ 2-5%ï¼‰

åŸå› ï¼šå¤§æ¨¡å‹éœ€è¦æ›´å¼ºçš„éçº¿æ€§èƒ½åŠ›ï¼ŒSwiGLU çš„é—¨æ§æœºåˆ¶æ›´æœ‰ç”¨ã€‚

### Q5: æˆ‘è¯¥æŒ‰ä»€ä¹ˆé¡ºåºå­¦ä¹ ï¼Ÿ

**A**: æ¨èé¡ºåºï¼š

1. âœ… **å¤ä¹ ç¬¬äºŒç« çš„ Mask å’Œ Multi-Head Attention**ï¼ˆ1 å¤©ï¼‰
2. âœ… **é‡ç‚¹ç†è§£ KV Cache**ï¼ˆ2-3 å¤©ï¼Œç”»å›¾ï¼ï¼‰
3. âœ… **äº†è§£ RoPE çš„ä½œç”¨**ï¼ˆ1 å¤©ï¼Œä¸éœ€è¦æ·±ç©¶æ•°å­¦ï¼‰
4. âœ… **å¿«é€Ÿäº†è§£ SwiGLU å’Œ RMSNorm**ï¼ˆåŠå¤©ï¼‰
5. âœ… **è¿è¡Œå®Œæ•´ä»£ç ç¤ºä¾‹**ï¼ˆ1 å¤©ï¼Œè°ƒè¯•ç†è§£ï¼‰
6. âœ… **å›å»çœ‹ç¬¬äº”ç« **ï¼ˆè¿™æ—¶å€™å°±çœ‹æ‡‚äº†ï¼ï¼‰

---

## ğŸ“š æ¨èèµ„æº

### è®ºæ–‡

- **RoPE**: [RoFormer: Enhanced Transformer with Rotary Position Embedding](https://arxiv.org/abs/2104.09864)
- **SwiGLU**: [GLU Variants Improve Transformer](https://arxiv.org/abs/2002.05202)
- **LLaMA**: [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971)
- **KV Cache**: æœç´¢ "KV Cache optimization" ç›¸å…³è®ºæ–‡

### ä»£ç 

- **LLaMA å®˜æ–¹å®ç°**: [facebookresearch/llama](https://github.com/facebookresearch/llama)
- **nanoGPT**: [karpathy/nanoGPT](https://github.com/karpathy/nanoGPT) (ç®€åŒ–ç‰ˆï¼Œé€‚åˆå­¦ä¹ )
- **Transformers åº“**: [huggingface/transformers](https://github.com/huggingface/transformers)

### è§†é¢‘

- **Andrej Karpathy - Let's build GPT**: [YouTube](https://www.youtube.com/watch?v=kCc8FmEb1nY)
- **3Blue1Brown - Attention in transformers**: [YouTube](https://www.youtube.com/watch?v=eMlx5fFNoYc)

---

## ğŸ“ æ€»ç»“

### ä» Transformer (ç¬¬äºŒç« ) åˆ° GPT (ç¬¬äº”ç« ) çš„æ¼”åŒ–

| ç»„ä»¶         | ç¬¬äºŒç« å­¦çš„           | ç¬¬äº”ç« ç”¨çš„   | æ ¸å¿ƒæ”¹è¿›     |
| ------------ | -------------------- | ------------ | ------------ |
| **æ¶æ„**     | Encoder + Decoder    | Decoder-only | ä¸“æ³¨ç”Ÿæˆä»»åŠ¡ |
| **æ³¨æ„åŠ›**   | Multi-Head Attention | + KV Cache   | ç”ŸæˆåŠ é€Ÿ 50x |
| **ä½ç½®ç¼–ç ** | Sinusoidal (åŠ æ³•)    | RoPE (æ—‹è½¬)  | æ›´å¥½çš„æ³›åŒ–   |
| **æ¿€æ´»å‡½æ•°** | ReLU                 | SwiGLU       | è¡¨è¾¾èƒ½åŠ›æ›´å¼º |
| **å½’ä¸€åŒ–**   | LayerNorm            | RMSNorm      | é€Ÿåº¦å¿« 10%   |

### æœ€é‡è¦çš„ä¸‰ä¸ªæ¦‚å¿µï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰

1. â­â­â­ **KV Cache**: ç†è§£ç”Ÿæˆè¿‡ç¨‹çš„æ ¸å¿ƒï¼Œå¿…é¡»æŒæ¡ï¼
2. â­â­ **Decoder-only æ¶æ„**: ä¸ºä»€ä¹ˆåªç”¨ Decoder
3. â­ **RoPE/SwiGLU/RMSNorm**: çŸ¥é“æ˜¯æ”¹è¿›ç‰ˆå°±è¡Œï¼Œç»†èŠ‚å¯ä»¥åç»­æ·±å…¥

### ä¸‹ä¸€æ­¥

ç°åœ¨ä½ å·²ç»æŒæ¡äº†æ‰€æœ‰å‰ç½®çŸ¥è¯†ï¼Œå¯ä»¥ï¼š

1. å›å»çœ‹ç¬¬äº”ç« çš„ä»£ç ï¼Œåº”è¯¥èƒ½çœ‹æ‡‚äº†
2. å°è¯•è¿è¡Œæœ¬æ–‡æ¡£çš„ä»£ç ç¤ºä¾‹
3. ä¿®æ”¹å‚æ•°ï¼Œè§‚å¯Ÿæ•ˆæœ
4. é˜…è¯» LLaMA æˆ– nanoGPT çš„æºç 

**åŠ æ²¹ï¼ä½ å·²ç»å…·å¤‡æ­å»ºå¤§æ¨¡å‹çš„åŸºç¡€äº†ï¼** ğŸš€

---

## ğŸ“ ç¬”è®°åŒºåŸŸ

> åœ¨è¿™é‡Œè®°å½•ä½ çš„å­¦ä¹ å¿ƒå¾—å’Œç–‘é—®ï¼š

```
æˆ‘çš„ç¬”è®°:
-


å¾…è§£å†³çš„é—®é¢˜:
-


ä»£ç å®éªŒè®°å½•:
-

```

---

**åˆ›å»ºæ—¥æœŸ**: 2025-11-05  
**æœ€åæ›´æ–°**: 2025-11-05  
**ç‰ˆæœ¬**: 1.0  
**åé¦ˆ**: å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œæ¬¢è¿æå‡ºï¼
