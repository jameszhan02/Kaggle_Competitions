{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e53bc7ef-fc1c-4737-89af-5857f71f46ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a baseline model with the minimum accuracy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b6df1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id  real_text_id\n",
      "0   0             1\n",
      "1   1             2\n",
      "2   2             1\n",
      "3   3             2\n",
      "4   4             2\n",
      "(95, 2)\n",
      "Index(['id', 'real_text_id'], dtype='object')\n",
      "id              0\n",
      "real_text_id    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# local train directory\n",
    "train_df = pd.read_csv('data/train.csv')\n",
    "# print the first 5 rows of the dataframe\n",
    "print(train_df.head())\n",
    "# print the shape of the dataframe\n",
    "print(train_df.shape)\n",
    "# print the columns of the dataframe\n",
    "print(train_df.columns)\n",
    "\n",
    "# check if there is any null data\n",
    "print(train_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bcd4710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['article_0000', 'article_0001', 'article_0002', 'article_0003', 'article_0004']\n"
     ]
    }
   ],
   "source": [
    "# Load train articles \n",
    "TRAIN_DIR = 'data/train'\n",
    "FILE1_NAME = 'file_1.txt'\n",
    "FILE2_NAME = 'file_2.txt'\n",
    "train_folders = os.listdir(TRAIN_DIR)\n",
    "\n",
    "# print first 5 folders name\n",
    "print(train_folders[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c137d4ec-c961-4e72-8f5e-25dafe15a2bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3    2\n",
      "Name: real_text_id, dtype: int64\n"
     ]
    }
   ],
   "source": [
    " real_artical_id = train_df.loc[train_df['id'] == 3, 'real_text_id']\n",
    "print(real_artical_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e26d7108-691a-4e23-9b6f-173d3e0219e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   article_id                                         file1_text  \\\n",
      "0           0  The VIRSA (Visible Infrared Survey Telescope A...   \n",
      "1           1  China\\nThe goal of this project involves achie...   \n",
      "2           2  Scientists can learn about how galaxies form a...   \n",
      "3           3  China\\nThe study suggests that multiple star s...   \n",
      "4           4  Dinosaur Rex was excited about his new toy set...   \n",
      "\n",
      "                                          file2_text  real_text_id  \n",
      "0  The China relay network has released a signifi...             1  \n",
      "1  The project aims to achieve an accuracy level ...             2  \n",
      "2  Dinosaur eggshells offer clues about what dino...             1  \n",
      "3  The importance for understanding how stars evo...             2  \n",
      "4  Analyzing how fast stars rotate within a galax...             2  \n",
      "95\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# loop through sub training folders and make data frame from them\n",
    "data = []\n",
    "for folder in train_folders:\n",
    "    current_folder_path = os.path.join(TRAIN_DIR, folder)\n",
    "    # double check if target path is a valid folder\n",
    "    if os.path.isdir(current_folder_path):\n",
    "        file_1_path = os.path.join(current_folder_path, FILE1_NAME)\n",
    "        file_2_path = os.path.join(current_folder_path, FILE2_NAME)\n",
    "\n",
    "        with open(file_1_path, 'r', encoding='utf-8') as f1, open(file_2_path, 'r', encoding='utf-8') as f2:\n",
    "            file_1_text = f1.read()\n",
    "            file_2_text = f2.read()\n",
    "\n",
    "        regex_folder_name = re.search(r'\\d+', folder).group()\n",
    "        article_id = int(regex_folder_name)\n",
    "        real_artical_id = train_df.loc[train_df['id'] == article_id, 'real_text_id'].values[0]\n",
    "\n",
    "        data.append({\n",
    "            'article_id': article_id,\n",
    "            'file1_text': file_1_text,\n",
    "            'file2_text': file_2_text,\n",
    "            'real_text_id': real_artical_id\n",
    "        })\n",
    "\n",
    "formated_data_df = pd.DataFrame(data)\n",
    "print(formated_data_df.head())\n",
    "print(len(formated_data_df))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "872ef79b-97db-4c78-ab19-3aefd0e50173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with test data\n",
    "TEST_DIR = 'data/test'\n",
    "test_folders = os.listdir(TEST_DIR)\n",
    "\n",
    "test_data = []\n",
    "for folder in test_folders:\n",
    "    current_folder_path = os.path.join(TEST_DIR, folder)\n",
    "    if os.path.isdir(current_folder_path):\n",
    "        file_1_path = os.path.join(current_folder_path, FILE1_NAME)\n",
    "        file_2_path = os.path.join(current_folder_path, FILE2_NAME)\n",
    "\n",
    "        with open(file_1_path, 'r', encoding='utf-8') as f1, open(file_2_path, 'r', encoding='utf-8') as f2:\n",
    "            file_1_text = f1.read()\n",
    "            file_2_text = f2.read()\n",
    "\n",
    "        regex_folder_name = re.search(r'\\d+', folder).group()\n",
    "        article_id = int(regex_folder_name)\n",
    "\n",
    "        test_data.append({\n",
    "            'article_id': article_id,\n",
    "            'file1_text': file_1_text,\n",
    "            'file2_text': file_2_text\n",
    "        })\n",
    "test_df = pd.DataFrame(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfb20066-0a02-484f-a384-921f9a2298b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Sheng\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Sheng\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['example', 'sentence', 'demonstrate', 'text', 'preprocessing']\n"
     ]
    }
   ],
   "source": [
    "# 1. text helper methods \n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "# [better] \n",
    "# 1. more words handling change all words back to it orginal form\n",
    "# 2. bypass all \"meaningless\" words\n",
    "def preprocess_text(text , remove_stopwords=True, lemmatize=True, stem=False):\n",
    "    # conver all text to lower case\n",
    "    text = text.lower()\n",
    "    # remain only char letters \n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # split words into array \n",
    "    words = text.split()\n",
    "    # remove stop words. \n",
    "    if remove_stopwords:\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        words = [word for word in words if word not in stop_words]\n",
    "    # to words orginal form\n",
    "    if lemmatize:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    \n",
    "    # words orginal form [ez version]\n",
    "    # if stem:\n",
    "    #     stemmer = PorterStemmer()\n",
    "    #     words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "    return words\n",
    "        \n",
    "# 示例用法\n",
    "text = \"This is an example sentence to demonstrate text preprocessing!\"\n",
    "processed_text = preprocess_text(text)\n",
    "print(processed_text)\n",
    "\n",
    "# 2. turn text to vector\n",
    "def text_to_vector(text, w2v_model):\n",
    "    tokens = preprocess_text(text)\n",
    "    \n",
    "    # fetch tokens vectors\n",
    "    vectors = []\n",
    "    for token in tokens:\n",
    "        if token in w2v_model.wv:\n",
    "            vectors.append(w2v_model.wv[token])\n",
    "    \n",
    "    # return average ｜ mean because longer text will sum bigger vector which is res we dont want \n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(w2v_model.wv.vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b82b46d4-2f7e-4e51-9c48-8f2890747a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Word2Vec model | and TF-IDF\n",
      "TF-IDF feature names: ['aan' 'aani' 'aaomega' ... 'zro' 'zu' 'zur']\n",
      "TF-IDF Dim（Vocab size）: 10100\n"
     ]
    }
   ],
   "source": [
    "# train the text 2 vector model with all texts we have \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# reorganize data real_text add label 1. fake add label 0. and text will be convert to vector with word2vec\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "print(\"training Word2Vec model | and TF-IDF\")\n",
    "all_texts = []\n",
    "for _, row in formated_data_df.iterrows():\n",
    "    all_texts.append(preprocess_text(row['file1_text']))\n",
    "    all_texts.append(preprocess_text(row['file2_text']))\n",
    "    \n",
    "for _, row in test_df.iterrows():\n",
    "    all_texts.append(preprocess_text(row['file1_text']))\n",
    "    all_texts.append(preprocess_text(row['file2_text']))\n",
    "\n",
    "\n",
    "w2v_model = Word2Vec(sentences=all_texts, vector_size=128, window=5, min_count=2, workers=4)\n",
    "\n",
    "all_sentences = [' '.join(text) for text in all_texts]\n",
    "# print(\"All sentences for TF-IDF:\", all_sentences)\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=3, max_df=0.95)\n",
    "tfidf_vectorizer.fit(all_sentences)\n",
    "# Debug: Print TF-IDF feature names\n",
    "print(\"TF-IDF feature names:\", tfidf_vectorizer.get_feature_names_out())\n",
    "vocab_size = len(tfidf_vectorizer.get_feature_names_out())\n",
    "print(f\"TF-IDF Dim（Vocab size）: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a72df14-76b1-4879-8af0-d2c8fb29464f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prepare_data(df):\n",
    "    # init empty array for hold data\n",
    "    data_list = []\n",
    "       \n",
    "    for _, row in df.iterrows():\n",
    "        article_id = row['article_id']\n",
    "        real_text_id = row.get('real_text_id', None)\n",
    "        # real txt\n",
    "        file1_vector = text_to_vector(row['file1_text'], w2v_model)\n",
    "        file1_tfidf = tfidf_vectorizer.transform([row['file1_text']]).toarray().flatten()\n",
    "        file2_vector = text_to_vector(row['file2_text'], w2v_model)\n",
    "        file2_tfidf = tfidf_vectorizer.transform([row['file2_text']]).toarray().flatten()\n",
    "        data_list.append({\n",
    "            'article_id': row['article_id'],\n",
    "            'file1_text': row['file1_text'],\n",
    "            'file1_vector': file1_vector,\n",
    "            'file1_tfidf': file1_tfidf,\n",
    "            'file2_vector': file2_vector,\n",
    "            'file2_tfidf': file2_tfidf,\n",
    "            'real_text_id': real_text_id\n",
    "        })\n",
    "    new_df = pd.DataFrame(data_list)\n",
    "\n",
    "    \n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736d4f1a-066b-4bce-9b3c-7994fd93f56a",
   "metadata": {},
   "source": [
    "### [better words hanlding] \n",
    "- 1. more words handling change all words back to it orginal form\n",
    "- 2. bypass all \"meaningless\" words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84605b60-005f-4b52-ac5e-37a49d7db1b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test content: 'This is a test sentence'\n",
      "output dim: (128,)\n",
      "output dim: [-0.03150508 -0.31145465  0.11689466  0.0894896   0.06723356]\n"
     ]
    }
   ],
   "source": [
    "# test out signal example \n",
    "test_text = \"This is a test sentence\"\n",
    "test_vector = text_to_vector(test_text, w2v_model)\n",
    "print(f\"test content: '{test_text}'\")\n",
    "print(f\"output dim: {test_vector.shape}\")\n",
    "print(f\"output dim: {test_vector[:5]}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2a16882-c7f4-46fc-bf89-5f3f12d23307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   article_id                                         file1_text  \\\n",
      "0           0  The VIRSA (Visible Infrared Survey Telescope A...   \n",
      "1           1  China\\nThe goal of this project involves achie...   \n",
      "2           2  Scientists can learn about how galaxies form a...   \n",
      "3           3  China\\nThe study suggests that multiple star s...   \n",
      "4           4  Dinosaur Rex was excited about his new toy set...   \n",
      "\n",
      "                                        file1_vector  \\\n",
      "0  [0.0230241, -0.38339213, 0.39330837, 0.2068413...   \n",
      "1  [0.07953804, -0.3028367, 0.42785868, 0.2871812...   \n",
      "2  [0.110884614, -0.2103085, 0.6307655, 0.3774753...   \n",
      "3  [0.08061906, -0.30437815, 0.41041118, 0.260557...   \n",
      "4  [0.08083893, -0.14955753, 0.1624579, 0.2090943...   \n",
      "\n",
      "                                         file1_tfidf  \\\n",
      "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "\n",
      "                                        file2_vector  \\\n",
      "0  [0.086444795, -0.31474155, 0.4457411, 0.274187...   \n",
      "1  [0.10372588, -0.24443209, 0.49126676, 0.324948...   \n",
      "2  [0.14753944, -0.06046359, 0.4031254, 0.1795347...   \n",
      "3  [0.14760503, -0.2141182, 0.570606, 0.36319402,...   \n",
      "4  [0.15121827, -0.104276314, 0.70981705, 0.39523...   \n",
      "\n",
      "                                         file2_tfidf  real_text_id  \n",
      "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...             1  \n",
      "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...             2  \n",
      "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...             1  \n",
      "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...             2  \n",
      "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...             2  \n"
     ]
    }
   ],
   "source": [
    "# get ready for traning\n",
    "final_train_df = prepare_data(formated_data_df)\n",
    "print(final_train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6d67716-ddf8-4d7b-ad9b-e88c3b05c790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare to train the data \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# 合并 file1 和 file2 的特征\n",
    "X_vectors = np.hstack((np.array(final_train_df['file1_vector'].tolist()), np.array(final_train_df['file2_vector'].tolist())))\n",
    "# X_tfidf = np.hstack((np.array(final_train_df['file1_tfidf'].tolist()), np.array(final_train_df['file2_tfidf'].tolist())))\n",
    "\n",
    "# double check size is same\n",
    "# assert X_vectors.shape[0] == X_tfidf.shape[0], \"size does not match\"\n",
    "# combin data \n",
    "# X_combined = np.hstack((X_vectors, X_tfidf))\n",
    "scaler = StandardScaler()\n",
    "X_combined_scaled = scaler.fit_transform(X_vectors)\n",
    "y = final_train_df['real_text_id'].values   \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined_scaled, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4b9e0f32-f7c6-427d-901f-0cd0c19c9dd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train by logisticRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "model = LogisticRegression()\n",
    "# model = BaggingClassifier(base_estimator=LogisticRegression(), n_estimators=10, random_state=42)\n",
    "selector = RFE(estimator=base_model, n_features_to_select=20) \n",
    "selector = selector.fit(X_train, y_train)\n",
    "selector.fit(X_train, y_train)\n",
    "X_train_filtered = selector.transform(X_train)\n",
    "X_test_filtered = selector.transform(X_test)\n",
    "# train\n",
    "model.fit(X_train_filtered, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "70410f1e-db47-4b07-9bb1-2c8e05980c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train by XGBoost\n",
    "# from xgboost import XGBClassifier\n",
    "# # Create xgboost classifier\n",
    "# model = XGBClassifier()\n",
    "# xgb_y_train = y_train - 1\n",
    "# # train start\n",
    "# model.fit(X_train, xgb_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ee64e9f7-4562-4d17-be7d-065b76f64eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 1 1 1 2 1 1 1 2 2 2 2 1 1 2 1 1 1]\n",
      "rate: 0.9474\n",
      "Cross-validation accuracy: 0.9053\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# 4. predict\n",
    "y_pred = model.predict(X_test_filtered)\n",
    "y_proba = model.predict_proba(X_test_filtered)\n",
    "# 5. see how accuracy it is \n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(y_pred)\n",
    "print(f\"rate: {accuracy:.4f}\")\n",
    "\n",
    "# 使用交叉验证\n",
    "cv_scores = cross_val_score(model, X_combined_scaled, y, cv=5)\n",
    "print(f\"Cross-validation accuracy: {cv_scores.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "79dc8247-af77-4002-8dd9-af247e806326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fake       1.00      0.83      0.91        12\n",
      "        Real       0.78      1.00      0.88         7\n",
      "\n",
      "    accuracy                           0.89        19\n",
      "   macro avg       0.89      0.92      0.89        19\n",
      "weighted avg       0.92      0.89      0.90        19\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nReport:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Fake', 'Real']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6727a787-b0e9-49a8-84a4-1e4bd6e4ec36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   article_id                                         file1_text  \\\n",
      "0           0  \"Music\" Music music music Music music Music mu...   \n",
      "1           1  underground exploration on SN's birth has prov...   \n",
      "2           2  This research aimed to understand how star sha...   \n",
      "3           3  Using OmegaCAM's wide field capabilities spann...   \n",
      "4           4  AssemblyCulture AssemblyCulture AssemblyCultur...   \n",
      "\n",
      "                                          file2_text  \n",
      "0  Since its launch on Paranal observatory's Very...  \n",
      "1  SN 1987A provides valuable insights as newer o...  \n",
      "2  ChromeDriver music player\\nThis study focused ...  \n",
      "3  greek translation :\\nvazhi (megaCAM), territor...  \n",
      "4  XClass is software tool that helps astronomers...  \n"
     ]
    }
   ],
   "source": [
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "96e0eeab-b62c-4b07-88e2-9ad84712510e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train the text 2 vector model with all texts we have \n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# # reorganize data real_text add label 1. fake add label 0. and text will be convert to vector with word2vec\n",
    "# from gensim.models import Word2Vec\n",
    "\n",
    "# print(\"training Word2Vec model | and TF-IDF\")\n",
    "# all_texts = []\n",
    "# for _, row in test_df.iterrows():\n",
    "#     all_texts.append(preprocess_text(row['file1_text']))\n",
    "#     all_texts.append(preprocess_text(row['file2_text']))\n",
    "\n",
    "# w2v_model = Word2Vec(sentences=all_texts, vector_size=128, window=5, min_count=2, workers=4)\n",
    "\n",
    "# all_sentences = [' '.join(text) for text in all_texts]\n",
    "# # print(\"All sentences for TF-IDF:\", all_sentences)\n",
    "\n",
    "# tfidf_vectorizer = TfidfVectorizer(min_df=3, max_df=0.95)\n",
    "# tfidf_vectorizer.fit(all_sentences)\n",
    "# # Debug: Print TF-IDF feature names\n",
    "# print(\"TF-IDF feature names:\", tfidf_vectorizer.get_feature_names_out())\n",
    "# vocab_size = len(tfidf_vectorizer.get_feature_names_out())\n",
    "# print(f\"TF-IDF Dim（Vocab size）: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d6f8bff4-9489-490c-a45e-d23b2fb8e069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   article_id                                         file1_text  \\\n",
      "0           0  \"Music\" Music music music Music music Music mu...   \n",
      "1           1  underground exploration on SN's birth has prov...   \n",
      "2           2  This research aimed to understand how star sha...   \n",
      "3           3  Using OmegaCAM's wide field capabilities spann...   \n",
      "4           4  AssemblyCulture AssemblyCulture AssemblyCultur...   \n",
      "\n",
      "                                        file1_vector  \\\n",
      "0  [0.08115593, -0.22142166, 0.4903126, 0.2310310...   \n",
      "1  [0.14342688, -0.24130912, 0.39748383, 0.294650...   \n",
      "2  [0.21412696, -0.12193324, 0.6276861, 0.3805165...   \n",
      "3  [0.08599428, -0.29641584, 0.49077696, 0.303334...   \n",
      "4  [0.112046205, -0.2529205, 0.29999223, 0.271616...   \n",
      "\n",
      "                                         file1_tfidf  \\\n",
      "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "\n",
      "                                        file2_vector  \\\n",
      "0  [0.08759738, -0.2952891, 0.44687837, 0.2439378...   \n",
      "1  [0.11217135, -0.28306866, 0.4770276, 0.3253647...   \n",
      "2  [0.109308034, -0.22175959, 0.45531157, 0.26355...   \n",
      "3  [-0.123486556, -0.5352284, 0.3967141, 0.222322...   \n",
      "4  [0.08833421, -0.37772545, 0.43085742, 0.299208...   \n",
      "\n",
      "                                         file2_tfidf real_text_id  \n",
      "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...         None  \n",
      "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...         None  \n",
      "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...         None  \n",
      "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...         None  \n",
      "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...         None  \n",
      "[2 2 1 ... 1 2 2]\n"
     ]
    }
   ],
   "source": [
    "final_test_df = prepare_data(test_df)\n",
    "print(final_test_df.head())\n",
    "X_test_vectors = np.hstack((np.array(final_test_df['file1_vector'].tolist()), np.array(final_test_df['file2_vector'].tolist())))\n",
    "# X_test_tfidf = np.hstack((np.array(final_test_df['file1_tfidf'].tolist()), np.array(final_test_df['file2_tfidf'].tolist())))\n",
    "# x_text_inputs = np.hstack((X_test_vectors, X_test_tfidf))\n",
    "X__test_combined_scaled = scaler.fit_transform(X_test_vectors)\n",
    "filiter_X__test_combined_scaled = selector.transform(X__test_combined_scaled)\n",
    "y_test_pred = model.predict(filiter_X__test_combined_scaled)\n",
    "print(y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3d95f6d1-1219-45b7-aac7-3366835897fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id  real_text_id\n",
      "0   0             2\n",
      "1   1             2\n",
      "2   2             1\n",
      "3   3             2\n",
      "4   4             2\n"
     ]
    }
   ],
   "source": [
    "submission_df = pd.DataFrame({\n",
    "    'id': final_test_df['article_id'],\n",
    "    'real_text_id': y_test_pred\n",
    "})\n",
    "print(submission_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f5afe9fc-33e6-42e6-8e85-fce701893a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfbe80c-4dfc-4fb8-8654-75985750f6c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
