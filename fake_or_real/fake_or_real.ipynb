{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e53bc7ef-fc1c-4737-89af-5857f71f46ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a baseline model with the minimum accuracy\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b6df1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id  real_text_id\n",
      "0   0             1\n",
      "1   1             2\n",
      "2   2             1\n",
      "3   3             2\n",
      "4   4             2\n",
      "(95, 2)\n",
      "Index(['id', 'real_text_id'], dtype='object')\n",
      "id              0\n",
      "real_text_id    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# local train directory\n",
    "train_df = pd.read_csv('data/train.csv')\n",
    "# print the first 5 rows of the dataframe\n",
    "print(train_df.head())\n",
    "# print the shape of the dataframe\n",
    "print(train_df.shape)\n",
    "# print the columns of the dataframe\n",
    "print(train_df.columns)\n",
    "\n",
    "# check if there is any null data\n",
    "print(train_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bcd4710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['article_0045', 'article_0042', 'article_0089', 'article_0074', 'article_0080']\n"
     ]
    }
   ],
   "source": [
    "# Load train articles \n",
    "TRAIN_DIR = 'data/train'\n",
    "FILE1_NAME = 'file_1.txt'\n",
    "FILE2_NAME = 'file_2.txt'\n",
    "train_folders = os.listdir(TRAIN_DIR)\n",
    "\n",
    "# print first 5 folders name\n",
    "print(train_folders[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c137d4ec-c961-4e72-8f5e-25dafe15a2bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3    2\n",
      "Name: real_text_id, dtype: int64\n"
     ]
    }
   ],
   "source": [
    " real_artical_id = train_df.loc[train_df['id'] == 3, 'real_text_id']\n",
    "print(real_artical_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e26d7108-691a-4e23-9b6f-173d3e0219e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   article_id                                          real_text  \\\n",
      "0          45  The VLT has enabled two major projects using t...   \n",
      "1          42  A key question is what causes powerful outflow...   \n",
      "2          89  The 2006 SPIE Symposium on Astronomical Telesc...   \n",
      "3          74  The primary mirror design of the European Extr...   \n",
      "4          80  The goal of this one-day workshop, part of the...   \n",
      "\n",
      "                                           fake_text  \n",
      "0  We have undertaken two major projects using th...  \n",
      "1  A burning question for us is what fuels the mo...  \n",
      "2  The 2006 SPIE Symposium on Astronomical Telesc...  \n",
      "3  The primary mirror design for the European Ext...  \n",
      "4  The goal of this one-day workshop, part of the...  \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# loop through sub training folders and make data frame from them\n",
    "data = []\n",
    "for folder in train_folders:\n",
    "    current_folder_path = os.path.join(TRAIN_DIR, folder)\n",
    "    # double check if target path is a vlid folder?\n",
    "    if(os.path.isdir(current_folder_path)):\n",
    "        file_1_path = os.path.join(current_folder_path, FILE1_NAME)\n",
    "        file_2_path = os.path.join(current_folder_path, FILE2_NAME)\n",
    "\n",
    "        with open(file_1_path, 'r') as f1, open(file_2_path, 'r') as f2:\n",
    "            file_1_text = f1.read()\n",
    "            file_2_text = f2.read()\n",
    "\n",
    "        regex_folder_name =  re.search(r'\\d+', folder).group()\n",
    "        article_id = int(regex_folder_name)\n",
    "        real_artical_id = train_df.loc[train_df['id'] == article_id, 'real_text_id'].values[0]\n",
    "\n",
    "        # distinguish real or fake text \n",
    "        real_text = file_1_text if real_artical_id == 1 else file_2_text\n",
    "        fake_text = file_2_text if real_artical_id == 1 else file_1_text\n",
    "\n",
    "        data.append({\n",
    "            'article_id': article_id,\n",
    "            'real_text': real_text,\n",
    "            'fake_text': fake_text\n",
    "        })\n",
    "        \n",
    "\n",
    "formated_data_df = pd.DataFrame(data)\n",
    "print(formated_data_df.head())\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7ea5091-11ec-4dd6-a238-f7996e15511c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reorganize data real_text add label 1. fake add label 0. and text will be convert to vector with word2vec\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a72df14-76b1-4879-8af0-d2c8fb29464f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df):\n",
    "    # init empty array for hold data\n",
    "    data_list = []\n",
    "       \n",
    "    for _, row in df.iterrows():\n",
    "        # real txt\n",
    "        real_vector = text_to_vector(row['real_text'], w2v_model)\n",
    "        data_list.append({\n",
    "            'text': row['real_text'],\n",
    "            'label': 1,\n",
    "            'vector': real_vector\n",
    "        })\n",
    "    \n",
    "        # fake txt\n",
    "        fake_vector = text_to_vector(row['fake_text'], w2v_model)\n",
    "        data_list.append({\n",
    "            'text': row['fake_text'],\n",
    "            'label': 0,\n",
    "            'vector': fake_vector\n",
    "        })\n",
    "\n",
    "    new_df = pd.DataFrame(data_list)\n",
    "\n",
    "    \n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bfb20066-0a02-484f-a384-921f9a2298b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. text helper methods \n",
    "def preprocess_text(text):\n",
    "    text = text.lower()                    \n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text) \n",
    "    return text.split()        \n",
    "\n",
    "\n",
    "# 2. turn text to vector\n",
    "def text_to_vector(text, w2v_model):\n",
    "    tokens = preprocess_text(text)\n",
    "    \n",
    "    # fetch tokens vectors\n",
    "    vectors = []\n",
    "    for token in tokens:\n",
    "        if token in w2v_model.wv:\n",
    "            vectors.append(w2v_model.wv[token])\n",
    "    \n",
    "    # return average ï½œ mean because longer text will sum bigger vector which is res we dont want \n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(w2v_model.wv.vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b82b46d4-2f7e-4e51-9c48-8f2890747a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Word2Vec model...\n"
     ]
    }
   ],
   "source": [
    "# train the text 2 vector model with all texts we have \n",
    "print(\"training Word2Vec model...\")\n",
    "all_texts = []\n",
    "for _, row in formated_data_df.iterrows():\n",
    "    all_texts.append(preprocess_text(row['real_text']))\n",
    "    all_texts.append(preprocess_text(row['fake_text']))\n",
    "\n",
    "w2v_model = Word2Vec(sentences=all_texts, vector_size=100, window=5, min_count=2, workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84605b60-005f-4b52-ac5e-37a49d7db1b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test content: 'This is a test sentence'\n",
      "output dim: (100,)\n",
      "output dim: [-0.65343744  0.7601629   0.35974774  0.20776053  0.42584202]\n"
     ]
    }
   ],
   "source": [
    "# test out signal example \n",
    "test_text = \"This is a test sentence\"\n",
    "test_vector = text_to_vector(test_text, w2v_model)\n",
    "print(f\"test content: '{test_text}'\")\n",
    "print(f\"output dim: {test_vector.shape}\")\n",
    "print(f\"output dim: {test_vector[:5]}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2a16882-c7f4-46fc-bf89-5f3f12d23307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label  \\\n",
      "0  The VLT has enabled two major projects using t...      1   \n",
      "1  We have undertaken two major projects using th...      0   \n",
      "2  A key question is what causes powerful outflow...      1   \n",
      "3  A burning question for us is what fuels the mo...      0   \n",
      "4  The 2006 SPIE Symposium on Astronomical Telesc...      1   \n",
      "\n",
      "                                              vector  \n",
      "0  [-0.3850595, 0.45181227, 0.21268378, 0.1195629...  \n",
      "1  [-0.41965276, 0.4923063, 0.2313726, 0.12997022...  \n",
      "2  [-0.39590812, 0.4650436, 0.2184925, 0.12260305...  \n",
      "3  [-0.4058296, 0.4762467, 0.22334301, 0.12556039...  \n",
      "4  [-0.38046274, 0.4477853, 0.20888482, 0.1177211...  \n"
     ]
    }
   ],
   "source": [
    "# get ready for traning\n",
    "final_train_df = prepare_data(formated_data_df)\n",
    "print(final_train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f2d02b6-3ab6-43b1-99d9-2d147554b73c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.3850595   0.45181227  0.21268378  0.11956295  0.2548367  -0.71222335\n",
      "  0.23883705  0.9614935  -0.2820338  -0.30678248 -0.19005585 -0.65788007\n",
      " -0.32269716  0.19566357  0.2604831  -0.1879813   0.21458021 -0.2789142\n",
      " -0.0717303  -0.74731326  0.3763354   0.08588213  0.46820936 -0.28876683\n",
      " -0.05159499  0.06134443 -0.38268653 -0.1530935  -0.17952447  0.02624926\n",
      "  0.71888906 -0.20242117 -0.04192891 -0.3702063  -0.06292815  0.5507588\n",
      "  0.21327262 -0.41083777 -0.28498262 -0.5013271  -0.01956422 -0.53746367\n",
      " -0.29426235  0.28117058  0.10457748 -0.10756782 -0.30622965  0.04533314\n",
      "  0.1767451   0.39287543  0.29109925 -0.3923705  -0.3925068   0.05960498\n",
      " -0.55001765  0.1300859   0.57544726 -0.41437635 -0.2881538   0.06690799\n",
      " -0.04587173  0.19370058 -0.34602267  0.1811748  -0.3648875   0.37097538\n",
      " -0.06311803  0.38362148 -0.18339378  0.32235017  0.02772361  0.3116253\n",
      "  0.28831902 -0.0807725   0.2720953   0.39508447 -0.19773069  0.13249268\n",
      " -0.06610598 -0.1225758  -0.18728998 -0.03222792  0.10202807  0.5432568\n",
      "  0.0473479  -0.19691078  0.04059918  0.25075066  0.58549166  0.22501406\n",
      "  0.47744638  0.00921854  0.38478282 -0.1492282   0.7772722   0.5828382\n",
      "  0.3968013  -0.57726717  0.3257529   0.12898877]\n"
     ]
    }
   ],
   "source": [
    "#check vector exist\n",
    "print(final_train_df['vector'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6d67716-ddf8-4d7b-ad9b-e88c3b05c790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare to train the data \n",
    "from sklearn.model_selection import train_test_split\n",
    "X = np.array(final_train_df['vector'].tolist())  # col to list\n",
    "y = final_train_df['label'].values        \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b9e0f32-f7c6-427d-901f-0cd0c19c9dd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee64e9f7-4562-4d17-be7d-065b76f64eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rate: 0.4737\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "# 4. predict\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# 5. see how accuracy it is \n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"rate: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "79dc8247-af77-4002-8dd9-af247e806326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fake       0.55      0.29      0.37        21\n",
      "        Real       0.44      0.71      0.55        17\n",
      "\n",
      "    accuracy                           0.47        38\n",
      "   macro avg       0.49      0.50      0.46        38\n",
      "weighted avg       0.50      0.47      0.45        38\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nReport:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Fake', 'Real']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dce6556-b215-4262-868f-f555796252ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
